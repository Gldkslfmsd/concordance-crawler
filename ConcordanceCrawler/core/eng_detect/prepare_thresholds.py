#!/usr/bin/env python
# -*- coding: utf-8 -*-

'''Prepares thresholds for is_english. It rewrites thresholds.py.
'''

from eng_detect import *

import nonenglish_samples
import english_samples
debug = False
def prepare_samples(slen,neng=nonenglish_samples.samples):
	nengall = " ".join(neng)
	engall = " ".join(english_samples.samples)
	sam = {
		"neng": [ nengall[i:i+slen] for i in range(0,len(nengall),slen) ],
		"eng": [ engall[i:i+slen] for i in range(0,len(engall),slen) ]
	}
	for k in list(sam.keys()):
		# cut the number of samples, because otherwise it's too slow
		sam[k] = sam[k][:300]
	return sam

def big_test(samples):
	det = EngDetector()
	similarities = {
		"neng": [],
		"eng": [],
	}
	for p in ("neng", "eng"):
		for s in samples[p]:
			similarities[p].append(
				det.englishness(s)
			)
	s = similarities
	f = open("similarities.py", "a")
	f.write("similarities = ")
	f.write(str(s)+"\n\n")
	f.close()
	print("neng mean",sum(s["neng"])/(1.0*len(s["neng"])))
	print("eng mean",sum(s["eng"])/(1.0*len(s["eng"])))
	return s

def germanic_samples():
	from samples import germanic_samples as s
	texts = []
	for l, t in s:
		texts.extend(t)
	return texts


def lower_iqr(numbers):
	s = sorted(numbers)
	n = len(s)
	a,b = s[n//4], s[n*3//4]
	return a-1.5*(b-a)


def gen_thresholds():
	lens = [
	    10, 20, 30, 40, 
		50, 60, 70, 80, 90, 100,
		150, 200, 300, 400, 
		500,
		1000, 1500, 2000, 5000
	]
	foreign_texts = germanic_samples()
	f = open("thresholds.py", "w")
	
	f.write("""'''Thresholds for EngDetector. Generated by eng_englishness.py,
(by function gen_threshold), can be changed manually. 

Threshold for "englishness" will be chosen by lenght of input text, because
the longer text, the better it fits to the vectors of frequencies of 1,2,3-grams in whole
English Wikipedia.

In comments are means similarities to English-Wikipedia vector of some
English and Non-English samples with given length.
'''\n\n\n""")

	f.write("thresholds = {\n")
	end = False
	try:
		for l in lens:
			sam = prepare_samples(l, neng=foreign_texts)
			print("length of samples:",l)
			sim = big_test(sam)
			nengmean = sum(sim["neng"])/(1.0*len(sim["neng"]))
			engmean = sum(sim["eng"])/(1.0*len(sim["eng"]))

			center = lower_iqr(sim["eng"])
			f.write(str(l)+": ")
			f.write(str(center) + ", # ")
			f.write("nengmean: "+str(nengmean)+" ")
			f.write("engmean: "+str(engmean)+" \n")
		end = True
	finally:
		f.write("}\n")
		if end:
			f.write("# Thresholds were generated correctly.\n")
		else:
			f.write("# Threshold generation was interrupted, the values can be incorrect!!!")
		f.close()
	

if __name__ == '__main__':
	gen_thresholds()


